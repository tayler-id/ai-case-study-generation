# Story 2.1: LLM Integration and Streaming

**Epic**: Epic 2: Case Study Generation & V1 Canvas  
**Story**: 2.1  
**Status**: âœ… COMPLETED

## Overview
Integrate Large Language Models (OpenAI GPT, Anthropic Claude) with streaming capabilities to generate case studies from collected project data in real-time.

## User Story
**As a user with collected project data**, I want to generate comprehensive case studies using AI models with real-time streaming so that I can see the analysis being created and monitor progress.

## Acceptance Criteria

### Backend Requirements
- [x] Implement LLM service with multi-provider support (OpenAI, Anthropic)
- [x] Add streaming response generation with Server-Sent Events (SSE)
- [x] Create case study generation templates and prompts
- [x] Implement token usage tracking and cost estimation
- [x] Add section detection and structured output formatting
- [x] Implement error handling and retry logic for LLM failures

### Frontend Requirements
- [x] Create streaming case study generator interface
- [x] Add real-time content display with markdown rendering
- [x] Implement progress tracking and cancellation support
- [x] Add model selection and configuration options
- [x] Show generation statistics and token usage
- [x] Provide export functionality for generated content

### API Endpoints
- `POST /case-study/generate/stream` - Stream case study generation
- `GET /case-study/models` - Get available LLM models
- `POST /case-study/generate` - Start background case study generation
- `GET /case-study/{id}/stream` - Stream progress for background generation

## Technical Implementation

### LLM Service Architecture
```python
# Multi-Provider LLM Service
class LLMService:
    - OpenAI GPT-4, GPT-3.5 Turbo integration
    - Anthropic Claude 3 Sonnet, Haiku integration
    - Streaming response handling with callbacks
    - Template-based prompt generation
    - Token usage tracking and cost estimation
    - Error handling and retry mechanisms
```

### Streaming Architecture
```python
# Server-Sent Events Streaming
- Real-time generation progress updates
- Section-based content streaming
- Error propagation to client
- Cancellation support
- Progress percentage tracking
```

### Frontend Components
```typescript
# Case Study Generator Components
- CaseStudyGenerator: Main generation interface
- CaseStudyStreamingDisplay: Real-time content display
- CaseStudyConfigModal: Model and template configuration
- Progress tracking with cancellation support
```

## Key Components

### 1. **LLM Integration**
   - Multi-provider abstraction layer
   - Streaming response handling
   - Template-based prompt generation
   - Token usage and cost tracking

### 2. **Streaming Infrastructure**
   - Server-Sent Events implementation
   - Real-time progress updates
   - Section-based content delivery
   - Error handling and recovery

### 3. **Case Study Templates**
   - Comprehensive business analysis
   - Technical project focus
   - Marketing campaign analysis
   - Product development insights

### 4. **Generation Management**
   - Background task processing
   - Progress tracking and cancellation
   - Database persistence of results
   - Export and sharing capabilities

## Dependencies
- Story 1.4: Real Data Integration (prerequisite)
- LangChain framework for LLM abstraction
- OpenAI API access and credentials
- Anthropic API access and credentials
- Server-Sent Events infrastructure

## Definition of Done
- [x] Users can generate case studies with streaming output
- [x] Multiple LLM models supported (GPT-4, GPT-3.5, Claude 3)
- [x] Real-time progress tracking and content display
- [x] Template selection affects generation structure and focus
- [x] Token usage and costs accurately tracked and displayed
- [x] Generated content can be exported and shared
- [x] Error scenarios handled gracefully with user feedback
- [x] Performance acceptable for typical case study generation
- [x] Unit tests cover LLM integration and streaming logic
- [x] Integration tests verify end-to-end generation flows

## Test Scenarios

### LLM Integration
1. **Multi-Model Support**: All configured models generate valid output
2. **Template Variation**: Different templates produce appropriate content
3. **Error Handling**: API failures handled gracefully with user feedback
4. **Token Tracking**: Usage and costs accurately calculated and displayed
5. **Rate Limiting**: API quota limits handled with proper backoff

### Streaming Functionality
1. **Real-Time Updates**: Content streams to client as generated
2. **Section Detection**: Content properly organized into sections
3. **Progress Tracking**: Accurate progress percentage throughout generation
4. **Cancellation**: Users can cancel generation mid-stream
5. **Error Recovery**: Stream errors handled without breaking UI

### User Experience
1. **Generation Flow**: Intuitive interface for starting case study generation
2. **Model Selection**: Clear options and descriptions for model choice
3. **Progress Feedback**: Clear indication of generation progress and status
4. **Content Display**: Generated content readable and well-formatted
5. **Export Options**: Easy export of completed case studies

## Performance Considerations
- **Streaming Efficiency**: Minimal latency between generation and display
- **Memory Management**: Handle large content generation without memory issues
- **Concurrent Users**: Support multiple simultaneous generations
- **Rate Limiting**: Respect API quotas while maintaining good UX
- **Caching**: Cache common templates and configurations

## Security Considerations
- **API Key Security**: Secure storage and handling of LLM API keys
- **Content Privacy**: Ensure user data privacy in LLM requests
- **Rate Limiting**: Prevent abuse and manage API costs
- **Content Filtering**: Basic content safety and appropriateness checks

## Configuration
- **Model Configuration**: Easy switching between development and production models
- **Template Management**: Configurable templates for different use cases
- **API Settings**: Configurable rate limits, timeouts, and retry logic
- **Cost Management**: Configurable budget alerts and usage limits

## Notes
- LLM API costs can be significant; implement usage monitoring
- Streaming responses require careful error handling to avoid broken states
- Different models have different strengths; provide guidance for model selection
- Template engineering is crucial for consistent, high-quality output
- Consider implementing content caching for common generation patterns